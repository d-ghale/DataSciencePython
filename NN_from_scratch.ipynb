{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detect handwritten digits using simple neural network\n",
    "\n",
    "Given an image of a handwritten digit stored as pixel values, build a model that can correctly identify the digit. \n",
    "\n",
    "We are implementing the process described in Chapter 1 of the book \"Introduction to Deep Learning\" by Eugene Charniak. \n",
    "\n",
    "\n",
    "## 1. Data Preparation and Transformation\n",
    "\n",
    "**1.1. Install Mnist dataset from Keras (http://yann.lecun.com/exdb/mnist/)** \n",
    "\n",
    "Input: each image of a digit is stored as 28 * 28 pixel values ranging from 0 to 255\n",
    "Output: actual value of each image is stored as the digit itself with 10 possible outcomes {0,1,2,...8,9}\n",
    "\n",
    "**1.2. Flatten input data** \n",
    "\n",
    "To simply the model, reshape two dimensional matrix (28 * 28) into one dimensional array (784 * 1)\n",
    "\n",
    "**1.3. Normalize input data** \n",
    "\n",
    "divide each value in the input array by the maximum value in the input array so that the values range from 0 to 1\n",
    "\n",
    "**1.4. Flatten output data** \n",
    "\n",
    "initialize the expected value as an array of 10 binary elements, 0 or 1 \n",
    "\n",
    "We will build separate model for each possible outcome\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# pip install keras tensorflow\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "import sklearn \n",
    "from sklearn import metrics\n",
    "from collections import Counter \n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_validate, y_validate) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plt_digit(x_data):\n",
    "    if x_data.shape[0] == 784:\n",
    "        x_data = x_data.reshape(28,28)\n",
    "    else:\n",
    "        x_data = x_data\n",
    "    plt.figure(figsize = (5,5))\n",
    "    \n",
    "    return plt.imshow(x_data,cmap=\"Greys\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plot an Example of Input values\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7ff2320e8f10>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Plot an Example of Input values\")\n",
    "plt_digit(x_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000,)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example of Output value:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Example of Output value:\")\n",
    "y_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_x(x_data, size1, size2):\n",
    "    new_data = x_data.reshape(size1, size2).astype('float32')\n",
    "    new_data /= np.max(x_data) #255\n",
    "    return new_data\n",
    "\n",
    "x_train_updated = normalize_x(x_train, 60000, 784)\n",
    "x_validate_updated = normalize_x(x_validate, 10000, 784)\n",
    "\n",
    "\n",
    "def flatten_y(y_data):\n",
    "    n_classes = len(Counter(y_data).keys()) \n",
    "    new_y_data = keras.utils.to_categorical(y_data, n_classes)\n",
    "    return new_y_data\n",
    "\n",
    "y_train_updated = flatten_y(y_train)\n",
    "y_validate_updated = flatten_y(y_validate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the number of occurances of each digits\n",
    "def count_unique(data):\n",
    "    unique, counts = np.unique(np.array(data), return_counts=True)\n",
    "    return dict(zip(unique, counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 5923, 1: 6742, 2: 5958, 3: 6131, 4: 5842, 5: 5421, 6: 5918, 7: 6265, 8: 5851, 9: 5949}\n",
      "{0.0: 540000, 1.0: 60000}\n"
     ]
    }
   ],
   "source": [
    "print(count_unique(y_train))\n",
    "print(count_unique(y_train_updated))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "print(\"Example of Normalized Input values\")\n",
    "x_train_updated[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# just a sanity check\n",
    "(x_train_updated[0] == x_train_updated[1]).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example of Flattened Output values\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 1., 0., 0., 0., 0.], dtype=float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Example of Flattened Output values\")\n",
    "y_train_updated[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Processing and Variables Initialization\n",
    "    \n",
    "**2.1. Split the data into batches** \n",
    "\n",
    "    60000 images into batch size of 100\n",
    "\n",
    "add why?\n",
    "\n",
    "\n",
    "**2.2. Initialize biases and weights** \n",
    "\n",
    "Randomly select values close to zero to prevent overflow in numpy package. If you select values close to 1 then logit becomes close to 1000, `e^1000` is too large for numpy to handle, so get infinity.\n",
    "      \n",
    "      number of biases = number of outputs = 10\n",
    "      number of weights = number of outputs times number of inputs = 10 * 784\n",
    "      \n",
    "For a single batch size 100,  \n",
    "100, 784 \n",
    "\n",
    "**2.3. Calculate logit function**   \n",
    "\n",
    "logit is a term for an un-normalized number that we are about to turn into probabilities using  softmax\n",
    "\n",
    "\n",
    "      pre_logit = bias + sum(weight*input)\n",
    "      number of logits for a single sample =  10\n",
    "      Inputs for sample vary so logits for sample also vary\n",
    "      number of logits for all sample =  10 * n_samples\n",
    "      \n",
    "      \n",
    "      \n",
    "**2.3. Calculate Softmax**\n",
    "\n",
    "Activation function \n",
    "\n",
    "Convert the pre_logits (unnormalized numbers) into probabilities using softmax function\n",
    "        pre_logit = {pre_logit_0, pre_logit_1,..., pre_logit_9}\n",
    "        prob_j = exp(pre_logit_j)/exp(sum(pre_logit)\n",
    "        where j is the jth element in pre_logit\n",
    "\n",
    "\n",
    "**3.3. Compute Cross Entropy Loss Function**\n",
    "\n",
    "Find the log probability of number using loss function\n",
    "\n",
    "        loss_func_j = -log(prob_j)\n",
    "    \n",
    "   why, \n",
    "   \n",
    "   \n",
    "   stochastic gradient descent.\n",
    "\n",
    "backward pass\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "add initial_bias in each element of 100 by 10 matrix\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle x and y at once with the same order\n",
    "x_train_updated_shuff = x_train_updated[:]\n",
    "y_train_updated_shuff = y_train_updated[:]\n",
    "y_train_shuff = y_train[:]\n",
    "\n",
    "import random\n",
    "\n",
    "mapIndexPosition = list(zip(x_train_updated_shuff, y_train_updated_shuff, y_train_shuff))\n",
    "random.Random(111).shuffle(mapIndexPosition)\n",
    "x_train_updated_shuff, y_train_updated_shuff, y_train_shuff = zip(*mapIndexPosition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7ff2217b3950>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt_digit(x_train_updated[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7ff2218a0510>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt_digit(x_train_updated_shuff[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.], dtype=float32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_updated_shuff[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_shuff[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_batch(data, size):\n",
    "    split_data= np.array([data[i:i + size] for i in range(0, len(data), size)], dtype=np.float128)\n",
    "    return split_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=100\n",
    "batch_x = split_batch(x_train_updated_shuff, batch_size)\n",
    "batch_y = split_batch(y_train_updated_shuff, batch_size)\n",
    "batch_actual = split_batch(y_train_shuff, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784)\n",
      "(600, 100, 784)\n"
     ]
    }
   ],
   "source": [
    "print(x_train_updated.shape)\n",
    "print(batch_x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7ff22190f710>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/IPython/core/pylabtools.py:132: UserWarning: Casting input data from 'float128' to 'float64' for imshow\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt_digit(batch_x[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 10)\n",
      "(600, 100, 10)\n",
      "(600, 100)\n"
     ]
    }
   ],
   "source": [
    "print(y_train_updated.shape)\n",
    "print(batch_y.shape)\n",
    "print(batch_actual.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.], dtype=float32)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_updated_shuff[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.], dtype=float128)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_y[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_shuff[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9.0"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_actual[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000\n",
      "784\n",
      "10\n",
      "600\n"
     ]
    }
   ],
   "source": [
    "n_train = x_train_updated.shape[0]\n",
    "n_input = x_train_updated.shape[1]\n",
    "n_output = y_train_updated.shape[1]\n",
    "batch_size = 100\n",
    "n_batch = round(len(x_train_updated)/batch_size)\n",
    "\n",
    "print(n_train)\n",
    "print(n_input)\n",
    "print(n_output)\n",
    "print(n_batch)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backward Pass\n",
    "\n",
    "For each batch (600 batches of size 100 each), take derivative of the loss with respect to bias then update bias by adding the sum of the derivative of biases to the initial bias. We have 100 by 10 biases. Now sum up the biases to obtain 10 biases. \n",
    "\n",
    "softmax_output = 100 by 10\n",
    "dBias is sum columnwise to get 10 \n",
    "\n",
    "updated_bias = initial_bias - learningrate * dBias\n",
    "updated_bias = (10,) \n",
    "\n",
    "\n",
    "Similarly, take derivative of the loss with respect to weight and update the weights. We have 600 * 10 * 784 weights, sum up the weights to obtain 10 * 784 weights. \n",
    "softmax_output = 100 by 10\n",
    "dloss_dlogit = 100 by 10\n",
    "\n",
    "initial_weight = 784 by 10 \n",
    "for single example: \n",
    "x = (784,)\n",
    "dloss_dlogit = (10,) \n",
    "dloss_dweight = (784, 10)\n",
    "\n",
    "for all example in the batch\n",
    "sum up to get\n",
    "x = (100, 784)\n",
    "x_t = (784, 100)\n",
    "dloss_dlogit = (100, 10) \n",
    "dloss_dweight = (100, 784, 10)\n",
    "\n",
    "updated_weight = initial_weight - learningrate * dWeights\n",
    "updated_weight = (784, 10)\n",
    "\n",
    "\n",
    "For second batch, redo forward pass using the new set of biases and weights, then backward pass to obtain new bias and weight. use it for third batch and so on. \n",
    "\n",
    "One epoch, once we go through all training example, compute accuracy. \n",
    "\n",
    "shuffle the batches + ys and use parameters obtained from last batch as  \n",
    "\n",
    "forward then backward until we get accuracy we want.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10,)\n",
      "[4.17022005e-04 7.20324493e-04 1.14374817e-07 3.02332573e-04\n",
      " 1.46755891e-04 9.23385948e-05 1.86260211e-04 3.45560727e-04\n",
      " 3.96767474e-04 5.38816734e-04]\n",
      "(10, 784)\n",
      "[[4.19194514e-04 6.85219500e-04 2.04452250e-04 ... 3.57511167e-04\n",
      "  3.30276937e-04 6.97368876e-04]\n",
      " [2.68650124e-04 8.08278014e-04 2.95288794e-04 ... 7.65183957e-04\n",
      "  5.68152543e-04 6.66188090e-04]\n",
      " [1.07813901e-04 8.42830514e-05 6.25121068e-04 ... 5.73561083e-04\n",
      "  9.05377380e-04 3.47413933e-04]\n",
      " ...\n",
      " [2.10073853e-04 2.26801116e-04 4.57388676e-04 ... 4.69515750e-04\n",
      "  3.84104101e-04 6.75198447e-04]\n",
      " [9.53945889e-05 1.61804875e-04 9.69143921e-04 ... 1.88520463e-05\n",
      "  4.72346213e-04 9.38995130e-04]\n",
      " [3.25574819e-04 3.22153019e-04 5.20307238e-05 ... 2.47718753e-04\n",
      "  1.23078052e-04 1.26350470e-04]]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "\n",
    "initial_b = np.random.uniform(0, 0.001, n_output)\n",
    "print(initial_b.shape)\n",
    "print(initial_b)\n",
    "\n",
    "initial_w = np.random.uniform(0, 0.001, n_output*n_input).reshape(10,784)\n",
    "print(initial_w.shape)\n",
    "print(initial_w)\n",
    "\n",
    "\n",
    "def forward_pass(data_x, bias_arr, weight_mat):\n",
    "    Z = np.array([bias_arr[i] + np.sum(data_x*weight_mat[i], axis =1) for i in range(n_output)], dtype=np.float128) #np.transpose\n",
    "    return Z\n",
    "\n",
    "def softmax(Z):\n",
    "    e_x = np.exp(np.transpose(Z))\n",
    "    S = np.array([e_x[i]/np.sum(e_x[i]) for i in range(e_x.shape[0])]) #e_x / np.sum(e_x)  \n",
    "    return S  \n",
    "\n",
    "def cross_entropy(data_y, data_y_pred):\n",
    "    pred_y = get_max_index(data_y_pred)\n",
    "    return -np.log(data)\n",
    "\n",
    "def get_max_index(data):\n",
    "    n_j = data.shape[0]\n",
    "    n_i = data.shape[1]\n",
    "    lst = np.array([list(data[j]).index(max(data[j])) for j in range(n_j)])\n",
    "    return lst\n",
    "\n",
    "def check_accuracy(predicted, expected):\n",
    "    batch_acc = sklearn.metrics.accuracy_score(predicted, expected)\n",
    "    return batch_acc\n",
    "\n",
    "def deriv_logit(actual, data):\n",
    "    lr_deriv_loss_lst = []\n",
    "    n_output = 10\n",
    "    for i in range(n_output):\n",
    "        lr_deriv_loss_lst.append(i)\n",
    "        prob_i = data[i]   \n",
    "        if actual == i:\n",
    "            lr_deriv_loss_lst[i] = 1- prob_i\n",
    "        else:\n",
    "            lr_deriv_loss_lst[i] = prob_i\n",
    "    return lr_deriv_loss_lst\n",
    "\n",
    "lr = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First Batch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is  0.08\n",
      "[7 6 0 6 2 0 0 2 2 0 2 2 7 2 1 2 7 0 7 1 6 7 0 2 1 2 2 2 4 1 0 7 6 4 4 0 6\n",
      " 0 1 6 0 2 0 7 2 2 0 7 7 2 2 6 0 4 0 6 8 0 0 0 0 0 2 2 7 1 0 7 2 7 2 4 4 1\n",
      " 2 0 7 2 0 2 1 7 7 6 6 7 7 1 4 6 2 0 1 0 2 0 0 2 7 2]\n",
      "[9. 6. 6. 6. 0. 0. 6. 8. 9. 8. 3. 3. 6. 0. 2. 9. 2. 2. 1. 7. 0. 0. 5. 1.\n",
      " 5. 8. 3. 4. 5. 2. 6. 5. 5. 7. 3. 4. 2. 4. 6. 3. 4. 8. 2. 9. 7. 1. 9. 8.\n",
      " 5. 0. 2. 7. 7. 7. 4. 1. 1. 5. 0. 5. 5. 8. 3. 0. 1. 6. 5. 1. 0. 1. 3. 7.\n",
      " 5. 6. 1. 5. 1. 0. 5. 7. 6. 9. 1. 1. 6. 5. 9. 6. 9. 6. 8. 2. 9. 5. 0. 8.\n",
      " 0. 0. 5. 7.]\n",
      "Initial biases are \n",
      " [4.17022005e-04 7.20324493e-04 1.14374817e-07 3.02332573e-04\n",
      " 1.46755891e-04 9.23385948e-05 1.86260211e-04 3.45560727e-04\n",
      " 3.96767474e-04 5.38816734e-04]\n",
      "Updated biases are \n",
      " [-2.0404271  -1.95977218 -1.6416114  -1.55802099 -1.3994152  -2.2787739\n",
      " -2.03980161 -1.7209771  -1.63863147 -1.71862089]\n"
     ]
    }
   ],
   "source": [
    "Zs_0 = forward_pass(batch_x[0], initial_b, initial_w) \n",
    "Ss_0 = softmax(Zs_0) \n",
    "pred_ys_0 = get_max_index(Ss_0)\n",
    "\n",
    "print(\"Accuracy is \", check_accuracy(pred_ys_0, batch_actual[0]))\n",
    "print(pred_ys_0)\n",
    "print(batch_actual[0])\n",
    "\n",
    "dLs_0 = np.array([deriv_logit(batch_actual[0][j], Ss_0[j]) for j in range(Ss_0.shape[0])])\n",
    "sum_dBs_0 = np.sum(dLs_0, axis = 0)\n",
    "updated_bias_0 = initial_b - lr*sum_dBs_0\n",
    "print(\"Initial biases are \\n\", initial_b)\n",
    "print(\"Updated biases are \\n\", updated_bias_0)\n",
    "\n",
    "\n",
    "sum_dWs_0 = np.sum(np.array([np.outer(dLs_0[i], batch_x[0][i]) for i in range(dLs_0.shape[0])]), axis = 0)\n",
    "#sum_dWs_0 = np.sum(np.array([np.outer(dLs_0, batch_x[0])]), axis = 0)\n",
    "updated_weights_0 = initial_w -lr*sum_dWs_0\n",
    "#print(\"Initial weights are \\n\", initial_w[0])\n",
    "#print(\"Updated weights are \\n\", updated_weights_0[0])\n",
    "#print(\"Check if initial weights equal to updated weights: \", all(initial_w[0] == updated_weights[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 784)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "initial_w.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 784)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum_dWs_0.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 784)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "updated_weights_0.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second Batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is  0.08\n",
      "[4 4 4 4 4 4 4 1 4 4 4 4 1 4 4 4 4 4 3 4 4 4 1 4 4 4 3 4 4 4 4 4 4 4 4 4 4\n",
      " 4 1 4 4 4 4 4 4 4 3 4 4 4 4 4 4 4 4 4 4 4 4 4 3 4 4 4 4 4 4 4 4 4 3 4 4 4\n",
      " 4 3 1 1 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 1 1]\n",
      "[9. 6. 6. 6. 0. 0. 6. 8. 9. 8. 3. 3. 6. 0. 2. 9. 2. 2. 1. 7. 0. 0. 5. 1.\n",
      " 5. 8. 3. 4. 5. 2. 6. 5. 5. 7. 3. 4. 2. 4. 6. 3. 4. 8. 2. 9. 7. 1. 9. 8.\n",
      " 5. 0. 2. 7. 7. 7. 4. 1. 1. 5. 0. 5. 5. 8. 3. 0. 1. 6. 5. 1. 0. 1. 3. 7.\n",
      " 5. 6. 1. 5. 1. 0. 5. 7. 6. 9. 1. 1. 6. 5. 9. 6. 9. 6. 8. 2. 9. 5. 0. 8.\n",
      " 0. 0. 5. 7.]\n",
      "Initial biases are \n",
      " [-2.0404271  -1.95977218 -1.6416114  -1.55802099 -1.3994152  -2.2787739\n",
      " -2.03980161 -1.7209771  -1.63863147 -1.71862089]\n",
      "Updated biases are \n",
      " [-1.09959904 -1.94964686 -1.03678578 -1.71228935 -8.56316088 -0.7999081\n",
      " -0.69994601 -1.1002523  -0.89994938 -0.60372292]\n"
     ]
    }
   ],
   "source": [
    "Zs_1 = forward_pass(batch_x[1], updated_bias_0, updated_weights_0) \n",
    "Ss_1 = softmax(Zs_1) \n",
    "pred_ys_1 = get_max_index(Ss_1)\n",
    "\n",
    "print(\"Accuracy is \", check_accuracy(pred_ys_1, batch_actual[1]))\n",
    "print(pred_ys_1)\n",
    "print(batch_actual[0])\n",
    "\n",
    "dLs_1 = np.array([deriv_logit(batch_actual[1][j], Ss_1[j]) for j in range(Ss_1.shape[0])])\n",
    "sum_dBs_1 = np.sum(dLs_1, axis = 0)\n",
    "updated_bias_1 = initial_b - lr*sum_dBs_1\n",
    "print(\"Initial biases are \\n\", updated_bias_0)\n",
    "print(\"Updated biases are \\n\", updated_bias_1)\n",
    "\n",
    "\n",
    "sum_dWs_1 = np.sum(np.array([np.outer(dLs_1[i], batch_x[1][i]) for i in range(dLs_1.shape[0])]), axis = 0)\n",
    "updated_weights_1 = initial_w -lr*sum_dWs_1\n",
    "#print(\"Initial weights are \\n\", updated_weights_0[0])\n",
    "#print(\"Updated weights are \\n\", updated_weights_1[0])\n",
    "#print(\"Check if initial weights equal to updated weights: \", all(initial_w[0] == updated_weights[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  1\n",
      "Accuracy of batch 0 : 0.08\n",
      "Accuracy of batch 1 : 0.08\n",
      "Accuracy of batch 2 : 0.01\n",
      "Accuracy of batch 3 : 0.02\n",
      "Accuracy of batch 4 : 0.0\n",
      "Accuracy of batch 5 : 0.0\n",
      "Accuracy of batch 6 : 0.06\n",
      "Accuracy of batch 7 : 0.0\n",
      "Accuracy of batch 8 : 0.01\n",
      "Accuracy of batch 9 : 0.0\n",
      "Accuracy of batch 10 : 0.01\n",
      "Accuracy of batch 11 : 0.0\n",
      "Accuracy of batch 12 : 0.0\n",
      "Accuracy of batch 13 : 0.0\n",
      "Accuracy of batch 14 : 0.0\n",
      "Accuracy of batch 15 : 0.01\n",
      "Accuracy of batch 16 : 0.01\n",
      "Accuracy of batch 17 : 0.01\n",
      "Accuracy of batch 18 : 0.0\n",
      "Accuracy of batch 19 : 0.0\n",
      "Accuracy of batch 20 : 0.01\n",
      "Accuracy of batch 21 : 0.01\n",
      "Accuracy of batch 22 : 0.0\n",
      "Accuracy of batch 23 : 0.0\n",
      "Accuracy of batch 24 : 0.0\n",
      "Accuracy of batch 25 : 0.0\n",
      "Accuracy of batch 26 : 0.0\n",
      "Accuracy of batch 27 : 0.0\n",
      "Accuracy of batch 28 : 0.0\n",
      "Accuracy of batch 29 : 0.0\n",
      "Accuracy of batch 30 : 0.01\n",
      "Accuracy of batch 31 : 0.01\n",
      "Accuracy of batch 32 : 0.0\n",
      "Accuracy of batch 33 : 0.0\n",
      "Accuracy of batch 34 : 0.0\n",
      "Accuracy of batch 35 : 0.0\n",
      "Accuracy of batch 36 : 0.01\n",
      "Accuracy of batch 37 : 0.0\n",
      "Accuracy of batch 38 : 0.0\n",
      "Accuracy of batch 39 : 0.0\n",
      "Accuracy of batch 40 : 0.0\n",
      "Accuracy of batch 41 : 0.01\n",
      "Accuracy of batch 42 : 0.0\n",
      "Accuracy of batch 43 : 0.0\n",
      "Accuracy of batch 44 : 0.0\n",
      "Accuracy of batch 45 : 0.0\n",
      "Accuracy of batch 46 : 0.0\n",
      "Accuracy of batch 47 : 0.0\n",
      "Accuracy of batch 48 : 0.0\n",
      "Accuracy of batch 49 : 0.0\n",
      "Accuracy of batch 50 : 0.01\n",
      "Accuracy of batch 51 : 0.0\n",
      "Accuracy of batch 52 : 0.0\n",
      "Accuracy of batch 53 : 0.0\n",
      "Accuracy of batch 54 : 0.0\n",
      "Accuracy of batch 55 : 0.01\n",
      "Accuracy of batch 56 : 0.01\n",
      "Accuracy of batch 57 : 0.01\n",
      "Accuracy of batch 58 : 0.0\n",
      "Accuracy of batch 59 : 0.01\n",
      "Accuracy of batch 60 : 0.0\n",
      "Accuracy of batch 61 : 0.0\n",
      "Accuracy of batch 62 : 0.0\n",
      "Accuracy of batch 63 : 0.01\n",
      "Accuracy of batch 64 : 0.0\n",
      "Accuracy of batch 65 : 0.02\n",
      "Accuracy of batch 66 : 0.01\n",
      "Accuracy of batch 67 : 0.0\n",
      "Accuracy of batch 68 : 0.0\n",
      "Accuracy of batch 69 : 0.0\n",
      "Accuracy of batch 70 : 0.0\n",
      "Accuracy of batch 71 : 0.0\n",
      "Accuracy of batch 72 : 0.0\n",
      "Accuracy of batch 73 : 0.0\n",
      "Accuracy of batch 74 : 0.0\n",
      "Accuracy of batch 75 : 0.0\n",
      "Accuracy of batch 76 : 0.0\n",
      "Accuracy of batch 77 : 0.01\n",
      "Accuracy of batch 78 : 0.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:18: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "nan is not in list",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-f32dee7888e3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0;31m#print(Ss)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mpred_ys\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_max_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0ml_pred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred_ys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-29-83dea74f0e94>\u001b[0m in \u001b[0;36mget_max_index\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0mn_j\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0mn_i\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m     \u001b[0mlst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_j\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mlst\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-29-83dea74f0e94>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0mn_j\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0mn_i\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m     \u001b[0mlst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_j\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mlst\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: nan is not in list"
     ]
    }
   ],
   "source": [
    "for n_epoch in range(3):\n",
    "    l_pred = []\n",
    "    print(\"Epoch \", n_epoch+1)\n",
    "    for n_batch in range(batch_x.shape[0]):\n",
    "        Zs = forward_pass(batch_x[n_batch], initial_b, initial_w) \n",
    "        Ss = softmax(Zs) \n",
    "        \n",
    "        #for i in range(batch_size)])\n",
    "        #print(np.sum(Ss, axis = 1)) \n",
    "        #print(\"Dimension of softmax output for all examples in a single batch is \", Ss.shape)\n",
    "        #print(Ss)\n",
    "\n",
    "        pred_ys = get_max_index(Ss)\n",
    "        l_pred.append(pred_ys)\n",
    "\n",
    "        #print(\"Dimension of predicted output for all examples in a single batch is \", pred_ys.shape)\n",
    "        print(\"Accuracy of batch\", n_batch, \":\",check_accuracy(pred_ys, batch_actual[n_batch]))\n",
    "        #print(pred_ys)\n",
    "        #print(batch_actual[n_batch])\n",
    "        # single example\n",
    "        # dLs = deriv_logit(batch_actual[0][0], Ss[0])\n",
    "        # all examples in the first batch (batch_actual[0])\n",
    "        # j is the jth training example in the first batch\n",
    "\n",
    "        dLs = np.array([deriv_logit(batch_actual[n_batch][j], Ss[j]) for j in range(Ss.shape[0])])\n",
    "        #print(batch_actual[n_batch][0])\n",
    "        #dLs_flat = dLs.flatten()\n",
    "        #print(np.all(dLs_flat <= 1)) #range(Ss.shape[0])))\n",
    "        #print(\"Dimension of derivative wrt logits for all examples in a single batch is \", dLs.shape)\n",
    "        #print(batch_actual[i].shape)\n",
    "        \n",
    "        # single example\n",
    "        #dBs = -lr*dLs[0]\n",
    "        # all examples\n",
    "        sum_dBs = np.sum(dLs, axis = 0)\n",
    "        #print(sum_dBs)\n",
    "        #dBs = -lr*dLs\n",
    "        #sum_dBs = np.sum(dBs, axis = 0)\n",
    "        \n",
    "        #print(\"Dimension of derivative wrt biases for all examples in a single batch is \", dBs.shape)\n",
    "        #print(\"Dimension of sum of dBs columnwise for all examples in a single batch is \", sum_dBs.shape)\n",
    "\n",
    "        updated_bias = initial_b - lr*sum_dBs\n",
    "        #print(\"Initial biases are \\n\", initial_b)\n",
    "        #print(\"Updated biases are \\n\", updated_bias)\n",
    "    \n",
    "\n",
    "        sum_dWs = np.sum(np.array([np.outer(dLs[i], batch_x[n_batch][i]) for i in range(dLs.shape[0])]), axis = 0)\n",
    "        #np.transpose(dLs).dot(batch_x[n_batch])\n",
    "        #print(\"Dimension of initial weights is \", initial_w.shape)\n",
    "        #print(\"Dimension of sum of dWs columnwise for all examples in a single batch is \", sum_dWs.shape)\n",
    "        \n",
    "        updated_weights = initial_w -lr*sum_dWs\n",
    "        #print(\"Initial weights are \\n\", initial_w[0])\n",
    "        #print(\"Updated weights are \\n\", updated_weights[0])\n",
    "        #print(\"Check if initial weights equal to updated weights: \", all(initial_w[0] == updated_weights[0]))\n",
    "        \n",
    "        #np.sum(np.transpose(dLs).dot(batch_x[0]), axis = 0)\n",
    "        #np.array([np.sum(batch_x[i]*dLs[n_sample][n_out], axis = 1) for n_sample in range(batch_x[i].shape[0]) for n_out in range(n_output)])\n",
    "        #np.transpose(batch_x[i]).dot(dLs) \n",
    "        #np.transpose(batch_x[i])*dLs \n",
    "        #print(np.mat(np.transpose(batch_x[i])) * np.mat(dLs))\n",
    "\n",
    "        initial_b = updated_bias\n",
    "        initial_w = updated_weights\n",
    "    if n_batch == 600-1:\n",
    "        print(\"Final accuracy is\", check_accuracy(np.array(l_pred).flatten(), batch_actual.flatten()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def deriv_bias(a, prob, lr = 0.001):\n",
    "    lr_deriv_loss_lst = []\n",
    "    n_output = 10\n",
    "    for i in range(n_output):\n",
    "        lr_deriv_loss_lst.append(i)\n",
    "        prob_i = prob[i]   \n",
    "        if a == i:\n",
    "            lr_deriv_loss_lst[i] = - lr*(prob_i - 1)\n",
    "        else:\n",
    "            lr_deriv_loss_lst[i] = - lr*(prob_i)\n",
    "    return lr_deriv_loss_lst\n",
    "\n",
    "def update_bias(X, size, Actuals, logits, old_bias, lr = 0.001):\n",
    "    updated_biases = []\n",
    "    for i in range(size):\n",
    "        batch_i = logits[size-1]\n",
    "        batch_i_a = Actuals[i]\n",
    "        batch_i_bias = np.array([deriv_bias(batch_i_a[i], batch_i[i], lr) for i in range(len(batch_i))])\n",
    "        updated_biases.append(old_bias + np.sum(batch_i_bias, axis = 0))\n",
    "    new_bias = np.sum(np.array(updated_biases), axis = 0)\n",
    "    return new_bias\n",
    "\n",
    "def update_weight(X, size, Actuals, logits, old_weights, lr = 0.001):\n",
    "    updated_weights = []\n",
    "    for i in range(size):\n",
    "        batch_i = logits[size-1]\n",
    "        batch_i_a = Actuals[i]\n",
    "        batch_i_bias = np.array([deriv_bias(batch_i_a[i], batch_i[i], lr) for i in range(len(batch_i))])\n",
    "        batch_i_weight =  np.array([batch_i_a[i]*batch_i_bias[i][j] for i in range(batch_size) for j in range(n_output)])\n",
    "        updated_weights.append(old_weights + np.sum(batch_i_weight, axis = 0))\n",
    "    new_weights = np.sum(np.array(updated_weights), axis = 0)\n",
    "    return new_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
