{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detect handwritten digits using simple neural network\n",
    "\n",
    "Given an image of a handwritten digit stored as pixel values, build a model that can correctly identify the digit. \n",
    "\n",
    "\n",
    "## 1. Data Preparation and Transformation\n",
    "\n",
    "**1.1. Install Mnist dataset from Keras (http://yann.lecun.com/exdb/mnist/)** \n",
    "\n",
    "Input: each image of a digit is stored as 28 * 28 pixel values ranging from 0 to 255\n",
    "Output: actual value of each image is stored as the digit itself with 10 possible outcomes {0,1,2,...8,9}\n",
    "\n",
    "**1.2. Flatten input data** \n",
    "\n",
    "To simply the model, reshape two dimensional matrix (28 * 28) into one dimensional array (784 * 1)\n",
    "\n",
    "**1.3. Normalize input data** \n",
    "\n",
    "divide each value in the input array by the maximum value in the input array so that the values range from 0 to 1\n",
    "\n",
    "**1.4. Flatten output data** \n",
    "\n",
    "initialize the expected value as an array of 10 binary elements, 0 or 1 \n",
    "\n",
    "We will build separate model for each possible outcome\n",
    "\n",
    "\n",
    "We are implementing the process described in the book chapter 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# pip install keras tensorflow\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "import sklearn \n",
    "from sklearn import metrics\n",
    "from collections import Counter \n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_validate, y_validate) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example of Input values\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   3,\n",
       "         18,  18,  18, 126, 136, 175,  26, 166, 255, 247, 127,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,  30,  36,  94, 154, 170,\n",
       "        253, 253, 253, 253, 253, 225, 172, 253, 242, 195,  64,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,  49, 238, 253, 253, 253, 253,\n",
       "        253, 253, 253, 253, 251,  93,  82,  82,  56,  39,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,  18, 219, 253, 253, 253, 253,\n",
       "        253, 198, 182, 247, 241,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,  80, 156, 107, 253, 253,\n",
       "        205,  11,   0,  43, 154,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,  14,   1, 154, 253,\n",
       "         90,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 139, 253,\n",
       "        190,   2,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  11, 190,\n",
       "        253,  70,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  35,\n",
       "        241, 225, 160, 108,   1,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         81, 240, 253, 253, 119,  25,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,  45, 186, 253, 253, 150,  27,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,  16,  93, 252, 253, 187,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0, 249, 253, 249,  64,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,  46, 130, 183, 253, 253, 207,   2,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  39,\n",
       "        148, 229, 253, 253, 253, 250, 182,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  24, 114, 221,\n",
       "        253, 253, 253, 253, 201,  78,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,  23,  66, 213, 253, 253,\n",
       "        253, 253, 198,  81,   2,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,  18, 171, 219, 253, 253, 253, 253,\n",
       "        195,  80,   9,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,  55, 172, 226, 253, 253, 253, 253, 244, 133,\n",
       "         11,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0, 136, 253, 253, 253, 212, 135, 132,  16,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0]], dtype=uint8)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Example of Input values\")\n",
    "x_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000,)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example of Output value:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Example of Output value:\")\n",
    "y_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fa0243df450>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize = (5,5))\n",
    "plt.imshow(x_train[0],cmap=\"Greys\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 5923,\n",
       " 1: 6742,\n",
       " 2: 5958,\n",
       " 3: 6131,\n",
       " 4: 5842,\n",
       " 5: 5421,\n",
       " 6: 5918,\n",
       " 7: 6265,\n",
       " 8: 5851,\n",
       " 9: 5949}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the number of occurances of each digits\n",
    "unique, counts = np.unique(np.array(y_train), return_counts=True)\n",
    "dict(zip(unique, counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_x(x_data, size1, size2):\n",
    "    new_data = x_data.reshape(size1, size2).astype('float32')\n",
    "    new_data /= np.max(x_data) #255\n",
    "    return new_data\n",
    "\n",
    "x_train_updated = normalize_x(x_train, 60000, 784)\n",
    "x_validate_updated = normalize_x(x_validate, 10000, 784)\n",
    "\n",
    "\n",
    "def flatten_y(y_data):\n",
    "    n_classes = len(Counter(y_data).keys()) \n",
    "    new_y_data = keras.utils.to_categorical(y_data, n_classes)\n",
    "    return new_y_data\n",
    "\n",
    "y_train_updated = flatten_y(y_train)\n",
    "y_validate_updated = flatten_y(y_validate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example of Normalized Input values\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.01176471, 0.07058824, 0.07058824,\n",
       "       0.07058824, 0.49411765, 0.53333336, 0.6862745 , 0.10196079,\n",
       "       0.6509804 , 1.        , 0.96862745, 0.49803922, 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.11764706, 0.14117648, 0.36862746, 0.6039216 ,\n",
       "       0.6666667 , 0.99215686, 0.99215686, 0.99215686, 0.99215686,\n",
       "       0.99215686, 0.88235295, 0.6745098 , 0.99215686, 0.9490196 ,\n",
       "       0.7647059 , 0.2509804 , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.19215687, 0.93333334,\n",
       "       0.99215686, 0.99215686, 0.99215686, 0.99215686, 0.99215686,\n",
       "       0.99215686, 0.99215686, 0.99215686, 0.9843137 , 0.3647059 ,\n",
       "       0.32156864, 0.32156864, 0.21960784, 0.15294118, 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.07058824, 0.85882354, 0.99215686, 0.99215686,\n",
       "       0.99215686, 0.99215686, 0.99215686, 0.7764706 , 0.7137255 ,\n",
       "       0.96862745, 0.94509804, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.3137255 , 0.6117647 , 0.41960785, 0.99215686, 0.99215686,\n",
       "       0.8039216 , 0.04313726, 0.        , 0.16862746, 0.6039216 ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.05490196,\n",
       "       0.00392157, 0.6039216 , 0.99215686, 0.3529412 , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.54509807,\n",
       "       0.99215686, 0.74509805, 0.00784314, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.04313726, 0.74509805, 0.99215686,\n",
       "       0.27450982, 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.13725491, 0.94509804, 0.88235295, 0.627451  ,\n",
       "       0.42352942, 0.00392157, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.31764707, 0.9411765 , 0.99215686, 0.99215686, 0.46666667,\n",
       "       0.09803922, 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.1764706 ,\n",
       "       0.7294118 , 0.99215686, 0.99215686, 0.5882353 , 0.10588235,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.0627451 , 0.3647059 ,\n",
       "       0.9882353 , 0.99215686, 0.73333335, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.9764706 , 0.99215686,\n",
       "       0.9764706 , 0.2509804 , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.18039216, 0.50980395,\n",
       "       0.7176471 , 0.99215686, 0.99215686, 0.8117647 , 0.00784314,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.15294118,\n",
       "       0.5803922 , 0.8980392 , 0.99215686, 0.99215686, 0.99215686,\n",
       "       0.98039216, 0.7137255 , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.09411765, 0.44705883, 0.8666667 , 0.99215686, 0.99215686,\n",
       "       0.99215686, 0.99215686, 0.7882353 , 0.30588236, 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.09019608, 0.25882354, 0.8352941 , 0.99215686,\n",
       "       0.99215686, 0.99215686, 0.99215686, 0.7764706 , 0.31764707,\n",
       "       0.00784314, 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.07058824, 0.67058825, 0.85882354,\n",
       "       0.99215686, 0.99215686, 0.99215686, 0.99215686, 0.7647059 ,\n",
       "       0.3137255 , 0.03529412, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.21568628, 0.6745098 ,\n",
       "       0.8862745 , 0.99215686, 0.99215686, 0.99215686, 0.99215686,\n",
       "       0.95686275, 0.52156866, 0.04313726, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.53333336, 0.99215686, 0.99215686, 0.99215686,\n",
       "       0.83137256, 0.5294118 , 0.5176471 , 0.0627451 , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        ], dtype=float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Example of Normalized Input values\")\n",
    "x_train_updated[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(x_train_updated[0] == x_train_updated[1]).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example of Flattened Output values\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 1., 0., 0., 0., 0.], dtype=float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Example of Flattened Output values\")\n",
    "y_train_updated[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Processing and Variables Initialization\n",
    "    \n",
    "**2.1. Split the data into batches** \n",
    "\n",
    "    60000 images into batch size of 100\n",
    "\n",
    "add why?\n",
    "\n",
    "\n",
    "**2.2. Initialize biases and weights** \n",
    "\n",
    "Randomly select values close to zero to prevent overflow in numpy package. If you select values close to 1 then logit becomes close to 1000, `e^1000` is too large for numpy to handle, so get infinity.\n",
    "      \n",
    "      number of biases = number of outputs = 10\n",
    "      number of weights = number of outputs times number of inputs = 10 * 784\n",
    "      \n",
    "For a single batch size 100,  \n",
    "100, 784 \n",
    "\n",
    "**2.3. Calculate logit function**   \n",
    "\n",
    "logit is a term for an un-normalized number that we are about to turn into probabilities using  softmax\n",
    "\n",
    "\n",
    "      pre_logit = bias + sum(weight*input)\n",
    "      number of logits for a single sample =  10\n",
    "      Inputs for sample vary so logits for sample also vary\n",
    "      number of logits for all sample =  10 * n_samples\n",
    "      \n",
    "      \n",
    "      \n",
    "**2.3. Calculate Softmax**\n",
    "\n",
    "Activation function \n",
    "\n",
    "Convert the pre_logits (unnormalized numbers) into probabilities using softmax function\n",
    "        pre_logit = {pre_logit_0, pre_logit_1,..., pre_logit_9}\n",
    "        prob_j = exp(pre_logit_j)/exp(sum(pre_logit)\n",
    "        where j is the jth element in pre_logit\n",
    "\n",
    "\n",
    "**3.3. Compute Cross Entropy Loss Function**\n",
    "\n",
    "Find the log probability of number using loss function\n",
    "\n",
    "        loss_func_j = -log(prob_j)\n",
    "    \n",
    "   why, \n",
    "   \n",
    "   \n",
    "   stochastic gradient descent.\n",
    "\n",
    "backward pass\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "add initial_bias in each element of 100 by 10 matrix\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_batch(data, size):\n",
    "    np.random.seed(111)\n",
    "    np.random.shuffle(data)\n",
    "    return np.array([data[i:i + size] for i in range(0, len(data), size)], dtype=np.float128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=100\n",
    "batch_x = split_batch(x_train_updated, batch_size)\n",
    "batch_y = split_batch(y_train_updated, batch_size)\n",
    "batch_actual = split_batch(np.array([y_train_updated[i].argmax(0) for i in range(len(y_train_updated))], dtype=np.float128), batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(600, 100, 784)\n",
      "(600, 100, 10)\n",
      "(600, 100)\n"
     ]
    }
   ],
   "source": [
    "print(batch_x.shape)\n",
    "print(batch_y.shape)\n",
    "print(batch_actual.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[9., 1., 4., ..., 6., 3., 3.],\n",
       "       [0., 3., 8., ..., 2., 2., 7.],\n",
       "       [1., 6., 7., ..., 8., 3., 9.],\n",
       "       ...,\n",
       "       [3., 9., 8., ..., 2., 3., 7.],\n",
       "       [4., 0., 9., ..., 4., 9., 6.],\n",
       "       [7., 5., 7., ..., 4., 1., 3.]], dtype=float128)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_actual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000\n",
      "784\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "n_train = x_train_updated.shape[0]\n",
    "n_input = x_train_updated.shape[1]\n",
    "n_output = y_train_updated.shape[1]\n",
    "print(n_train)\n",
    "print(n_input)\n",
    "print(n_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10,)\n",
      "[4.17022005e-03 7.20324493e-03 1.14374817e-06 3.02332573e-03\n",
      " 1.46755891e-03 9.23385948e-04 1.86260211e-03 3.45560727e-03\n",
      " 3.96767474e-03 5.38816734e-03]\n",
      "(10, 784)\n",
      "[[0.00419195 0.0068522  0.00204452 ... 0.00357511 0.00330277 0.00697369]\n",
      " [0.0026865  0.00808278 0.00295289 ... 0.00765184 0.00568153 0.00666188]\n",
      " [0.00107814 0.00084283 0.00625121 ... 0.00573561 0.00905377 0.00347414]\n",
      " ...\n",
      " [0.00210074 0.00226801 0.00457389 ... 0.00469516 0.00384104 0.00675198]\n",
      " [0.00095395 0.00161805 0.00969144 ... 0.00018852 0.00472346 0.00938995]\n",
      " [0.00325575 0.00322153 0.00052031 ... 0.00247719 0.00123078 0.0012635 ]]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "\n",
    "initial_b = np.random.uniform(0, 0.01, n_output)\n",
    "print(initial_b.shape)\n",
    "print(initial_b)\n",
    "\n",
    "initial_w = np.random.uniform(0, 0.01, n_output*n_input).reshape(10,784)\n",
    "print(initial_w.shape)\n",
    "print(initial_w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_pass(data_x, bias_arr, weight_mat):\n",
    "    Z = np.transpose(np.array([bias_arr[i] + np.sum(data_x*weight_mat[i], axis =1) for i in range(n_output)], dtype=np.float128))\n",
    "    return Z\n",
    "\n",
    "def softmax(Z):\n",
    "    e_x = np.exp(Z)\n",
    "    S = e_x / np.sum(e_x)  \n",
    "    #np.array([e_x[i]/np.sum(np.exp(e_x[i])) for i in range(e_x.shape(0))])\n",
    "    #\n",
    "    return S  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.41836419, 0.41212477, 0.44043061, 0.36102954, 0.41962403,\n",
       "        0.39120909, 0.43043009, 0.41678099, 0.37500109, 0.38132131],\n",
       "       [0.21570351, 0.24740677, 0.24516983, 0.26791225, 0.23850148,\n",
       "        0.22436942, 0.22957211, 0.22945395, 0.23970548, 0.24572707],\n",
       "       [0.45396016, 0.48970234, 0.45732948, 0.43185141, 0.44410275,\n",
       "        0.42289709, 0.44462606, 0.43822526, 0.42413199, 0.41138736],\n",
       "       [0.3753365 , 0.34629404, 0.35991941, 0.33666171, 0.34391666,\n",
       "        0.34215218, 0.34470335, 0.37107461, 0.36814629, 0.3379293 ],\n",
       "       [0.53283834, 0.62922685, 0.55841468, 0.55148599, 0.56555826,\n",
       "        0.50628576, 0.55605688, 0.56340562, 0.53955495, 0.5308367 ],\n",
       "       [0.76148862, 0.79265711, 0.72869726, 0.70032181, 0.74617058,\n",
       "        0.67865071, 0.76316797, 0.75512827, 0.74218342, 0.72391008],\n",
       "       [0.35660468, 0.37663074, 0.38196929, 0.35485904, 0.34331449,\n",
       "        0.34205683, 0.35684283, 0.39696752, 0.3754105 , 0.36041478],\n",
       "       [0.4832405 , 0.45012474, 0.46048616, 0.43048472, 0.49014848,\n",
       "        0.42988807, 0.4463442 , 0.4635718 , 0.43077036, 0.42817278],\n",
       "       [0.86260527, 0.84606223, 0.86414364, 0.81561761, 0.82810649,\n",
       "        0.79308095, 0.90240606, 0.87805468, 0.86558905, 0.85584222],\n",
       "       [0.75336759, 0.76833724, 0.76221835, 0.70646553, 0.71371087,\n",
       "        0.66799954, 0.76539666, 0.71734296, 0.69894618, 0.7144797 ],\n",
       "       [0.49767185, 0.43124333, 0.47977756, 0.46201076, 0.45248735,\n",
       "        0.45955845, 0.45760447, 0.47694569, 0.47887872, 0.45186858],\n",
       "       [0.50760435, 0.49809505, 0.52443701, 0.46581279, 0.51666094,\n",
       "        0.50692878, 0.50600521, 0.50147526, 0.4644424 , 0.4880674 ],\n",
       "       [0.49415574, 0.46039976, 0.51112909, 0.46334898, 0.51985731,\n",
       "        0.4708174 , 0.49417105, 0.49142941, 0.45729119, 0.48402877],\n",
       "       [0.32348436, 0.31920951, 0.35483431, 0.29331221, 0.33681193,\n",
       "        0.3037702 , 0.34047916, 0.29930929, 0.31453092, 0.292682  ],\n",
       "       [0.22387759, 0.23464073, 0.26348242, 0.21878277, 0.26060966,\n",
       "        0.22073749, 0.24981805, 0.27451592, 0.25222305, 0.25406401],\n",
       "       [0.31759162, 0.29283101, 0.31531976, 0.32120266, 0.31386366,\n",
       "        0.30454705, 0.31189327, 0.3358144 , 0.31771791, 0.31707981],\n",
       "       [0.39033009, 0.37763639, 0.39877242, 0.35966803, 0.36011797,\n",
       "        0.36671724, 0.38423756, 0.38635648, 0.38246208, 0.35332139],\n",
       "       [0.59753751, 0.63077547, 0.62228481, 0.57056475, 0.61615082,\n",
       "        0.60327512, 0.64580378, 0.61375308, 0.58877167, 0.61148314],\n",
       "       [0.3875074 , 0.40313661, 0.35819812, 0.36279782, 0.36475261,\n",
       "        0.32049453, 0.37666338, 0.41125079, 0.35007336, 0.35802916],\n",
       "       [0.54244738, 0.43224346, 0.5261779 , 0.45467359, 0.50900088,\n",
       "        0.44084544, 0.48071871, 0.50925736, 0.50937452, 0.48233517],\n",
       "       [0.42461982, 0.42355784, 0.44160781, 0.39365696, 0.42875223,\n",
       "        0.39014846, 0.42838013, 0.46168401, 0.37406403, 0.3994217 ],\n",
       "       [0.46315983, 0.42815042, 0.46105502, 0.44918438, 0.43290119,\n",
       "        0.43900688, 0.45333157, 0.46367497, 0.45788401, 0.43079023],\n",
       "       [0.49864313, 0.52277059, 0.48125103, 0.45736848, 0.48348286,\n",
       "        0.45880631, 0.45424336, 0.51361914, 0.43577434, 0.46053345],\n",
       "       [0.59058723, 0.52769235, 0.5743355 , 0.56953754, 0.5438133 ,\n",
       "        0.5574051 , 0.55366331, 0.58001296, 0.52821441, 0.55118045],\n",
       "       [0.51890501, 0.50404215, 0.57276725, 0.4859087 , 0.52095487,\n",
       "        0.52396017, 0.55558373, 0.53645112, 0.51530812, 0.50934774],\n",
       "       [0.69439457, 0.72739032, 0.73181548, 0.66345892, 0.65794529,\n",
       "        0.67231231, 0.63762617, 0.70369001, 0.64364012, 0.64180698],\n",
       "       [0.52438948, 0.46219826, 0.53358486, 0.48863691, 0.51640171,\n",
       "        0.49948517, 0.52701712, 0.53309529, 0.50109305, 0.50607771],\n",
       "       [0.65859052, 0.56520241, 0.65393793, 0.59371953, 0.63501128,\n",
       "        0.5947262 , 0.60630366, 0.6245666 , 0.64953325, 0.60339177],\n",
       "       [0.61129514, 0.54420698, 0.57550549, 0.5621185 , 0.54952723,\n",
       "        0.55114798, 0.57778016, 0.57062953, 0.59707638, 0.52732037],\n",
       "       [0.74164738, 0.72159474, 0.75543075, 0.70042217, 0.73951066,\n",
       "        0.7141236 , 0.74133966, 0.73536963, 0.69556792, 0.74436608],\n",
       "       [0.76420755, 0.71969154, 0.76942349, 0.71102592, 0.74432737,\n",
       "        0.70047239, 0.7568652 , 0.77218272, 0.71513942, 0.70840481],\n",
       "       [0.46112657, 0.42450159, 0.44301725, 0.41934042, 0.41194477,\n",
       "        0.42951795, 0.46864402, 0.4478737 , 0.44678768, 0.42924054],\n",
       "       [0.4239803 , 0.36017475, 0.41176875, 0.3646186 , 0.41269454,\n",
       "        0.41287699, 0.40827106, 0.41294809, 0.3964096 , 0.37553734],\n",
       "       [0.43869186, 0.41766634, 0.44565135, 0.4010181 , 0.4523541 ,\n",
       "        0.40950416, 0.42758264, 0.43897612, 0.40240776, 0.42285472],\n",
       "       [0.3051269 , 0.31278769, 0.3262481 , 0.29906088, 0.32542653,\n",
       "        0.28589068, 0.31659167, 0.32206631, 0.29100044, 0.30907049],\n",
       "       [0.33147217, 0.33009127, 0.33142514, 0.30787729, 0.35641065,\n",
       "        0.30177976, 0.32455456, 0.32138566, 0.30342638, 0.33105161],\n",
       "       [0.6489445 , 0.64578606, 0.62271677, 0.60019112, 0.5966072 ,\n",
       "        0.58865975, 0.6179384 , 0.62879298, 0.5938437 , 0.61724815],\n",
       "       [1.06829322, 1.00740593, 1.08247387, 1.00718177, 1.00580797,\n",
       "        0.98727245, 0.98495562, 1.05672717, 1.01515361, 0.99056742],\n",
       "       [0.51328631, 0.49340461, 0.47879661, 0.50406102, 0.44576106,\n",
       "        0.46401645, 0.45203496, 0.52654184, 0.48464329, 0.46698156],\n",
       "       [0.59081422, 0.49773352, 0.6244843 , 0.5259479 , 0.60917561,\n",
       "        0.53116126, 0.53428929, 0.57006445, 0.57880553, 0.56513257],\n",
       "       [0.49216371, 0.46696565, 0.50087385, 0.4629254 , 0.5005702 ,\n",
       "        0.47069817, 0.49387287, 0.49564148, 0.44995624, 0.48087633],\n",
       "       [0.63279217, 0.57769001, 0.66034662, 0.58742488, 0.59984013,\n",
       "        0.60606297, 0.58690575, 0.59373596, 0.6192255 , 0.58308236],\n",
       "       [0.65553528, 0.55607717, 0.61775058, 0.5952924 , 0.58595622,\n",
       "        0.59585867, 0.64619069, 0.6160977 , 0.59808282, 0.6083655 ],\n",
       "       [0.69876153, 0.5893287 , 0.6529802 , 0.61843965, 0.67587233,\n",
       "        0.59501638, 0.60771414, 0.66909069, 0.62619645, 0.61862261],\n",
       "       [0.32564755, 0.31597287, 0.35011425, 0.31080869, 0.32345866,\n",
       "        0.33293201, 0.33558517, 0.33891734, 0.32857076, 0.3137191 ],\n",
       "       [0.56809361, 0.58771341, 0.59610987, 0.5732015 , 0.55737358,\n",
       "        0.55371653, 0.57846543, 0.56647296, 0.54121968, 0.55813127],\n",
       "       [0.46632952, 0.48499663, 0.47683339, 0.46911025, 0.47228393,\n",
       "        0.47628435, 0.44914161, 0.48833636, 0.46119364, 0.48538148],\n",
       "       [0.71193   , 0.62825574, 0.67223484, 0.61144192, 0.6897104 ,\n",
       "        0.65377176, 0.61856842, 0.67616505, 0.63116242, 0.626715  ],\n",
       "       [0.42399402, 0.38677767, 0.46297285, 0.4432577 , 0.45834702,\n",
       "        0.43329985, 0.40411038, 0.40846513, 0.42222633, 0.42375457],\n",
       "       [0.50029313, 0.47607425, 0.51038665, 0.48579509, 0.47994751,\n",
       "        0.49430705, 0.47020963, 0.51519462, 0.45711497, 0.48375086],\n",
       "       [0.60472492, 0.60131129, 0.63873917, 0.58478021, 0.62283711,\n",
       "        0.59615302, 0.59313584, 0.61680692, 0.58220423, 0.58100858],\n",
       "       [0.60622509, 0.58149541, 0.53936013, 0.56106578, 0.54788647,\n",
       "        0.54734768, 0.57040406, 0.59629408, 0.5256744 , 0.55039411],\n",
       "       [0.5513926 , 0.50449416, 0.55915385, 0.53168456, 0.5533183 ,\n",
       "        0.53342289, 0.57873584, 0.53693349, 0.54039605, 0.57531413],\n",
       "       [0.66403787, 0.61072673, 0.64349018, 0.62011154, 0.61394621,\n",
       "        0.62647875, 0.63242127, 0.65787548, 0.59644724, 0.61844626],\n",
       "       [0.68566488, 0.68839234, 0.66744535, 0.65497031, 0.65010451,\n",
       "        0.6152968 , 0.68598514, 0.70003552, 0.68722615, 0.66601564],\n",
       "       [0.44592937, 0.41158499, 0.47992296, 0.40041671, 0.4509181 ,\n",
       "        0.43154332, 0.43242652, 0.44825147, 0.44688632, 0.40566429],\n",
       "       [0.64040342, 0.65576064, 0.6487273 , 0.63323379, 0.60295184,\n",
       "        0.62216558, 0.62840793, 0.67687884, 0.63494793, 0.62038856],\n",
       "       [0.26743991, 0.29170988, 0.32828125, 0.27163727, 0.31638769,\n",
       "        0.27336189, 0.30706185, 0.31447749, 0.31626548, 0.29898801],\n",
       "       [0.20695613, 0.2238863 , 0.24198906, 0.21927604, 0.24259067,\n",
       "        0.22286196, 0.23368021, 0.24272021, 0.24632715, 0.23862556],\n",
       "       [0.71631749, 0.69020447, 0.75690407, 0.67002592, 0.74413402,\n",
       "        0.67257454, 0.65941688, 0.7305624 , 0.6750804 , 0.71017831],\n",
       "       [0.53148954, 0.54173402, 0.52103155, 0.47661284, 0.53274988,\n",
       "        0.49730725, 0.47466125, 0.53777536, 0.51638506, 0.48711492],\n",
       "       [0.5481669 , 0.53478819, 0.53519334, 0.5154955 , 0.50204928,\n",
       "        0.50325757, 0.50237764, 0.54205686, 0.4709113 , 0.51481402],\n",
       "       [0.27713708, 0.2440968 , 0.27059991, 0.26770664, 0.24935834,\n",
       "        0.2634455 , 0.27961234, 0.26625766, 0.27394424, 0.26089334],\n",
       "       [0.43204796, 0.40787884, 0.44992703, 0.39838045, 0.44243537,\n",
       "        0.4241694 , 0.42589306, 0.45001674, 0.41439147, 0.39108388],\n",
       "       [0.72917041, 0.70948985, 0.75647081, 0.6895405 , 0.69561339,\n",
       "        0.68223104, 0.72588187, 0.75708791, 0.66197594, 0.72005269],\n",
       "       [0.51643993, 0.4901735 , 0.52332757, 0.45960583, 0.52439085,\n",
       "        0.48548122, 0.5026476 , 0.47048917, 0.51364174, 0.50859521],\n",
       "       [0.28563715, 0.29830742, 0.31197102, 0.27864253, 0.28046122,\n",
       "        0.28855566, 0.30678646, 0.31882969, 0.2901272 , 0.28329161],\n",
       "       [0.53217561, 0.55118519, 0.57082388, 0.56305622, 0.5569819 ,\n",
       "        0.56423251, 0.57731074, 0.55014022, 0.51426159, 0.56702594],\n",
       "       [0.45415272, 0.42535892, 0.42021997, 0.40092467, 0.41379989,\n",
       "        0.41541327, 0.40219674, 0.42745827, 0.40913872, 0.40450885],\n",
       "       [0.4491568 , 0.50690646, 0.49738988, 0.44762221, 0.53619179,\n",
       "        0.44928055, 0.47961663, 0.48798837, 0.45262311, 0.46979305],\n",
       "       [0.65684611, 0.65635115, 0.68845734, 0.64250178, 0.66071285,\n",
       "        0.68173506, 0.70950582, 0.66835151, 0.65902788, 0.68057803],\n",
       "       [0.57921259, 0.49206754, 0.5461016 , 0.51370685, 0.52189777,\n",
       "        0.5322542 , 0.55438969, 0.55049956, 0.54189538, 0.53619699],\n",
       "       [0.50639288, 0.51803639, 0.57325698, 0.51892126, 0.61034292,\n",
       "        0.50909464, 0.57375407, 0.52038294, 0.53910906, 0.54177465],\n",
       "       [0.337833  , 0.29133278, 0.34513323, 0.31206614, 0.33880003,\n",
       "        0.33252846, 0.33477954, 0.3062171 , 0.34980229, 0.31483435],\n",
       "       [0.39610885, 0.37030245, 0.41804137, 0.39609953, 0.40177273,\n",
       "        0.37536376, 0.40878276, 0.40130624, 0.40572222, 0.38923974],\n",
       "       [0.47019798, 0.44890576, 0.45537774, 0.4074183 , 0.4525447 ,\n",
       "        0.44165027, 0.47494147, 0.48357456, 0.47992231, 0.45779512],\n",
       "       [0.37280417, 0.34284009, 0.3640112 , 0.36015722, 0.35280718,\n",
       "        0.38017605, 0.33222966, 0.38108736, 0.35615838, 0.35472152],\n",
       "       [0.57896527, 0.52421758, 0.57326183, 0.51374997, 0.55782891,\n",
       "        0.5382909 , 0.5505524 , 0.54902146, 0.53197002, 0.53516159],\n",
       "       [0.39860237, 0.38222787, 0.42592869, 0.38835805, 0.42426619,\n",
       "        0.37071463, 0.42267795, 0.37476893, 0.39120118, 0.38306948],\n",
       "       [0.47517456, 0.46356356, 0.43014422, 0.42010073, 0.44573452,\n",
       "        0.42892029, 0.38264551, 0.45757659, 0.41900404, 0.40804261],\n",
       "       [0.63096634, 0.60264699, 0.65460486, 0.59076028, 0.61200495,\n",
       "        0.58086431, 0.58894962, 0.62344497, 0.56769991, 0.59308897],\n",
       "       [0.87438453, 0.80994915, 0.79216567, 0.7755639 , 0.77401945,\n",
       "        0.77427175, 0.83537502, 0.85577802, 0.82988974, 0.80885618],\n",
       "       [0.40797163, 0.3749872 , 0.40775205, 0.37503658, 0.41811439,\n",
       "        0.40756045, 0.40296577, 0.42200606, 0.39654523, 0.3791273 ],\n",
       "       [0.45620104, 0.4715377 , 0.47090611, 0.423231  , 0.51649059,\n",
       "        0.4336654 , 0.47895748, 0.48720317, 0.4609712 , 0.44416236],\n",
       "       [0.4845237 , 0.44170456, 0.51023116, 0.45290642, 0.47025568,\n",
       "        0.45698787, 0.47226509, 0.47706444, 0.48149452, 0.4644209 ],\n",
       "       [0.41187963, 0.36333295, 0.36588001, 0.37592874, 0.37779494,\n",
       "        0.36280277, 0.38763247, 0.40515514, 0.36936436, 0.36402235],\n",
       "       [0.4148138 , 0.41021652, 0.43900715, 0.37287257, 0.4348472 ,\n",
       "        0.39173882, 0.38238539, 0.41187942, 0.3954241 , 0.39091627],\n",
       "       [0.32990839, 0.39020234, 0.35535572, 0.34787685, 0.36178784,\n",
       "        0.36087854, 0.3681085 , 0.33898366, 0.31438054, 0.35004638],\n",
       "       [0.40375427, 0.3853291 , 0.42930099, 0.3687278 , 0.38172097,\n",
       "        0.3817661 , 0.38522795, 0.41540738, 0.38061796, 0.38093508],\n",
       "       [0.5763287 , 0.50865731, 0.53747083, 0.5053682 , 0.50635365,\n",
       "        0.49746047, 0.55754174, 0.53130507, 0.51639385, 0.52174187],\n",
       "       [0.12113693, 0.12963855, 0.1473609 , 0.12723724, 0.14633753,\n",
       "        0.13520967, 0.13349011, 0.14496433, 0.15400221, 0.15166767],\n",
       "       [0.50598407, 0.58406535, 0.53182916, 0.51958822, 0.5515391 ,\n",
       "        0.49302727, 0.49444657, 0.56348183, 0.5123348 , 0.51055239],\n",
       "       [0.55795103, 0.48517457, 0.4933277 , 0.4745746 , 0.51045012,\n",
       "        0.49085711, 0.48814299, 0.51022176, 0.47903065, 0.45179289],\n",
       "       [0.33579391, 0.32009855, 0.35187114, 0.31241817, 0.34888172,\n",
       "        0.3584534 , 0.32858929, 0.35101847, 0.32547967, 0.33934277],\n",
       "       [0.32454736, 0.32648946, 0.35680103, 0.31793034, 0.3337758 ,\n",
       "        0.31615582, 0.35711242, 0.32985977, 0.32874197, 0.31061961],\n",
       "       [0.31553506, 0.31625425, 0.35633534, 0.3209267 , 0.3170537 ,\n",
       "        0.32502948, 0.35473529, 0.33469705, 0.33587068, 0.31669202],\n",
       "       [0.14607573, 0.16158242, 0.17957362, 0.15856055, 0.17542986,\n",
       "        0.15607284, 0.16660528, 0.16635895, 0.18256552, 0.15935023],\n",
       "       [0.58127734, 0.54356368, 0.57748084, 0.51325449, 0.55003816,\n",
       "        0.55162908, 0.55378211, 0.57008522, 0.50619471, 0.53452492],\n",
       "       [1.0214176 , 0.94783305, 1.04141466, 0.93963245, 0.97171509,\n",
       "        0.95600772, 0.95481167, 0.99594141, 0.96455913, 0.98144482],\n",
       "       [0.9050096 , 0.81499744, 0.89256033, 0.81358901, 0.79885716,\n",
       "        0.80991243, 0.85866279, 0.84153895, 0.80036464, 0.83261436]],\n",
       "      dtype=float128)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forward_pass(batch_x[0], initial_b, initial_w) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float128)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_x[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(784, 10)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.transpose(initial_w).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_x[0]*np.transpose(initial_w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(batch_x[0]*np.transpose(initial_w), axis =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(initial_w*initial_w[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Zs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-889b88b87de9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mZs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mZs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Zs' is not defined"
     ]
    }
   ],
   "source": [
    "print(Zs.shape)\n",
    "Zs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Ss.shape)\n",
    "Ss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_max_index(data):\n",
    "    n_j = data.shape[0]\n",
    "    n_i = data.shape[1]\n",
    "    lst = np.array([list(data[j]).index(max(data[j])) for j in range(n_j)])\n",
    "    return lst\n",
    "\n",
    "def cross_entropy(data_y, data_y_pred):\n",
    "    pred_y = get_max_index(data_y_pred)\n",
    "    \n",
    "    return -np.log(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 784)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_x[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_accuracy(predicted, expected):\n",
    "    batch_acc = sklearn.metrics.accuracy_score(predicted, expected)\n",
    "    return batch_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([9., 1., 4., 7., 1., 5., 5., 5., 3., 4., 2., 0., 7., 3., 0., 2., 4.,\n",
       "       6., 3., 8., 6., 4., 7., 7., 6., 4., 8., 4., 5., 2., 7., 8., 6., 3.,\n",
       "       3., 1., 3., 2., 3., 1., 3., 0., 6., 0., 5., 2., 3., 5., 7., 8., 5.,\n",
       "       8., 2., 2., 8., 5., 5., 4., 1., 7., 0., 1., 3., 2., 5., 2., 6., 8.,\n",
       "       2., 8., 4., 5., 4., 6., 4., 0., 4., 4., 1., 3., 5., 4., 0., 9., 7.,\n",
       "       4., 5., 9., 1., 5., 3., 7., 6., 7., 9., 7., 1., 6., 3., 3.],\n",
       "      dtype=float128)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_actual.shape\n",
    "batch_actual[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "600\n"
     ]
    }
   ],
   "source": [
    "n_batch = round(len(x_train_updated)/batch_size)\n",
    "print(n_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(600, 100)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_actual.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(600, 100, 784)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float128)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_x[0]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def get_max_index(data):\n",
    "    n_j = data.shape[0]\n",
    "    n_i = data.shape[1]\n",
    "    lst = []\n",
    "    for j in range(n_j):\n",
    "        for i in range(n_i):\n",
    "            lst.append(list(data[j][i]).index(max(data[j][i])))\n",
    "    return np.reshape(lst, (n_j, n_i))    \n",
    "\n",
    "#np.array([sum_wxbte[i]/sum2_wxbte[i] for i in range(len(data_x))])\n",
    "#sum_wxb = np.array([bias_arr[i] + sum_wx[j] for i in range(n_output) for j in range(n_output)])    \n",
    "#sum_wxbt = np.transpose(sum_wx)\n",
    "#sum_wxbte = np.exp(sum_wxbt)\n",
    "#sum2_wxbte = np.sum(sum_wxbte, axis=1)\n",
    "\n",
    "def get_logit(, ):\n",
    "    sum_wx = np.array([bias_arr[i] + np.sum(data_x*weight_mat[i], axis =1) for i in range(n_output)], dtype=np.float128)\n",
    "    \n",
    "    return logit\n",
    "\n",
    "all_logits = np.array([get_logit(initial_b, initial_w, batch_x[i]) for i in range(n_batch)])\n",
    "print(x_train_updated.shape)\n",
    "print(all_logits.shape)\n",
    "print(all_logits[0])\n",
    "all_logits\n",
    "\n",
    "def check_accuracy(predicted, expected):\n",
    "    batch_acc = []\n",
    "    for i in range(expected.shape[0]):\n",
    "        batch_acc.append(sklearn.metrics.accuracy_score(predicted[i], expected[i]))\n",
    "    return sum(batch_acc)/len(batch_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cross_entropy(all_logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_batch_x_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_actual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backward Pass\n",
    "\n",
    "For each batch (600 batches of size 100 each), take derivative of the loss with respect to bias then update bias by adding the sum of the derivative of biases to the initial bias. We have 100 by 10 biases. Now sum up the biases to obtain 10 biases. \n",
    "\n",
    "softmax_output = 100 by 10\n",
    "dBias is sum columnwise to get 10 \n",
    "\n",
    "updated_bias = initial_bias - learningrate * dBias\n",
    "updated_bias = (10,) \n",
    "\n",
    "\n",
    "Similarly, take derivative of the loss with respect to weight and update the weights. We have 600 * 10 * 784 weights, sum up the weights to obtain 10 * 784 weights. \n",
    "softmax_output = 100 by 10\n",
    "dloss_dlogit = 100 by 10\n",
    "\n",
    "initial_weight = 784 by 10 \n",
    "for single example: \n",
    "x = (784,)\n",
    "dloss_dlogit = (10,) \n",
    "dloss_dweight = (784, 10)\n",
    "\n",
    "for all example in the batch\n",
    "sum up to get\n",
    "x = (100, 784)\n",
    "x_t = (784, 100)\n",
    "dloss_dlogit = (100, 10) \n",
    "dloss_dweight = (100, 784, 10)\n",
    "\n",
    "updated_weight = initial_weight - learningrate * dWeights\n",
    "updated_weight = (784, 10)\n",
    "\n",
    "\n",
    "For second batch, redo forward pass using the new set of biases and weights, then backward pass to obtain new bias and weight. use it for third batch and so on. \n",
    "\n",
    "One epoch, once we go through all training example, compute accuracy. \n",
    "\n",
    "shuffle the batches + ys and use parameters obtained from last batch as  \n",
    "\n",
    "forward then backward until we get accuracy we want.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimension of softmax output for all examples in a single batch is  (100, 10)\n",
      "Dimension of predicted output for all examples in a single batch is  (100,)\n",
      "Accuracy is  0.08\n",
      "Dimension of derivative wrt logits for all examples in a single batch is  (100, 10)\n",
      "Dimension of derivative wrt biases for all examples in a single batch is  (100, 10)\n",
      "Dimension of sum of dBs columnwise for all examples in a single batch is  (10,)\n",
      "Initial biases are  [4.17022005e-03 7.20324493e-03 1.14374817e-06 3.02332573e-03\n",
      " 1.46755891e-03 9.23385948e-04 1.86260211e-03 3.45560727e-03\n",
      " 3.96767474e-03 5.38816734e-03]\n",
      "Updated biases are  [0.07504695 0.09803843 0.10080175 0.14372476 0.14218475 0.14162164\n",
      " 0.09268813 0.11424793 0.08479745 0.04630895]\n",
      "[-0. -0. -0. -0. -0. -0. -0. -0. -0. -0.]\n",
      "Dimension of softmax output for all examples in a single batch is  (100, 10)\n",
      "Dimension of predicted output for all examples in a single batch is  (100,)\n",
      "Accuracy is  0.12\n",
      "Dimension of derivative wrt logits for all examples in a single batch is  (100, 10)\n",
      "Dimension of derivative wrt biases for all examples in a single batch is  (100, 10)\n",
      "Dimension of sum of dBs columnwise for all examples in a single batch is  (10,)\n",
      "Initial biases are  [4.17022005e-03 7.20324493e-03 1.14374817e-06 3.02332573e-03\n",
      " 1.46755891e-03 9.23385948e-04 1.86260211e-03 3.45560727e-03\n",
      " 3.96767474e-03 5.38816734e-03]\n",
      "Updated biases are  [0.07504751 0.09801148 0.10082798 0.14370373 0.14219241 0.14163906\n",
      " 0.09269144 0.11424397 0.08480324 0.04630502]\n",
      "[-0. -0. -0. -0. -0. -0. -0. -0. -0. -0.]\n"
     ]
    }
   ],
   "source": [
    "def deriv_logit(actual, data):\n",
    "    lr_deriv_loss_lst = []\n",
    "    n_output = 10\n",
    "    for i in range(n_output):\n",
    "        lr_deriv_loss_lst.append(i)\n",
    "        prob_i = data[i]   \n",
    "        if actual == i:\n",
    "            lr_deriv_loss_lst[i] = 1- prob_i\n",
    "        else:\n",
    "            lr_deriv_loss_lst[i] = prob_i\n",
    "    return lr_deriv_loss_lst\n",
    "\n",
    "\n",
    "for i in range(2):\n",
    "    Zs = forward_pass(batch_x[i], initial_b, initial_w) \n",
    "    Ss = softmax(Zs) #for i in range(batch_size)])\n",
    "    print(\"Dimension of softmax output for all examples in a single batch is \", Ss.shape)\n",
    "    #print(Ss)\n",
    "    \n",
    "    pred_ys = get_max_index(Ss)\n",
    "    print(\"Dimension of predicted output for all examples in a single batch is \", pred_ys.shape)\n",
    "    print(\"Accuracy is \", check_accuracy(pred_ys, batch_actual[i]))\n",
    "    \n",
    "    # single example\n",
    "    # dLs = deriv_logit(batch_actual[0][0], Ss[0])\n",
    "    # all examples in the first batch (batch_actual[0])\n",
    "    # j is the jth training example in the first batch\n",
    "    \n",
    "    dLs = np.array([deriv_logit(batch_actual[0][j], Ss[j]) for j in range(Ss.shape[0])])\n",
    "    print(\"Dimension of derivative wrt logits for all examples in a single batch is \", dLs.shape)\n",
    "    #print(batch_actual[i].shape)\n",
    "    lr = 0.01\n",
    "    \n",
    "    # single example\n",
    "    #dBs = -lr*dLs[0]\n",
    "    # all examples\n",
    "    dBs = -lr*dLs\n",
    "    print(\"Dimension of derivative wrt biases for all examples in a single batch is \", dBs.shape)\n",
    "    sum_dBs = np.sum(dBs, axis = 0)\n",
    "    print(\"Dimension of sum of dBs columnwise for all examples in a single batch is \", sum_dBs.shape)\n",
    "    #print(sum_dBs)\n",
    "    \n",
    "    updated_bias = initial_b - sum_dBs\n",
    "    print(\"Initial biases are \", initial_b)\n",
    "    print(\"Updated biases are \", updated_bias)\n",
    "    \n",
    "    \n",
    "    #print(np.mat(np.transpose(batch_x[i])) * np.mat(dLs))\n",
    "    #print(batch_x[i][0])\n",
    "    dWs = -lr*np.transpose(batch_x[i]).dot(dLs) #np.transpose(batch_x[i])*dLs \n",
    "    print(dWs[0])\n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.63921571, 0.91764706, 0.51372552, 0.51372552,\n",
       "       0.51372552, 0.51372552, 0.51372552, 0.36078432, 0.02745098,\n",
       "       0.01568628, 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.60392159,\n",
       "       0.95686275, 0.99607843, 0.99607843, 0.99607843, 0.99607843,\n",
       "       0.99607843, 0.99607843, 0.99607843, 0.82745099, 0.32941177,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.07843138, 0.43921569,\n",
       "       0.43921569, 0.43921569, 0.43921569, 0.43921569, 0.43921569,\n",
       "       0.60392159, 0.98823529, 0.60392159, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.03137255, 0.50588238, 0.94117647,\n",
       "       0.60392159, 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.09803922,\n",
       "       0.63921571, 0.99607843, 0.95294118, 0.53333336, 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.24705882, 0.92156863, 0.99607843, 0.99607843,\n",
       "       0.32156864, 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.24705882, 0.91764706,\n",
       "       0.99607843, 0.99607843, 0.68627453, 0.05098039, 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.25098041, 0.9137255 , 0.99607843, 0.99607843, 0.99607843,\n",
       "       0.73725492, 0.68235296, 0.18039216, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.14117648, 0.9137255 , 0.99607843,\n",
       "       0.99607843, 0.99607843, 0.99607843, 0.99607843, 0.99607843,\n",
       "       0.60392159, 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.19607843, 0.99607843, 0.99607843, 0.81176472, 0.75294119,\n",
       "       0.36078432, 0.75686276, 0.99607843, 0.60392159, 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.05098039, 0.58823532,\n",
       "       0.37254903, 0.05882353, 0.        , 0.        , 0.71372551,\n",
       "       0.99607843, 0.60392159, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.25098041, 0.96078432, 0.99607843, 0.46666667,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.71372551,\n",
       "       0.99607843, 0.54509807, 0.02352941, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.65098041, 0.96470588, 0.87843138, 0.09411765,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.09019608, 0.73725492, 0.96862745,\n",
       "       0.88235295, 0.41568628, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.09019608,\n",
       "       0.78039217, 0.99607843, 0.88235295, 0.41568628, 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.08235294, 0.78431374, 0.99607843, 0.99607843,\n",
       "       0.26274511, 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.07843138, 0.78823531,\n",
       "       0.99607843, 0.89019608, 0.1882353 , 0.01568628, 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.58431375,\n",
       "       0.95294118, 0.96078432, 0.99607843, 0.66666669, 0.16470589,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.13333334, 1.        , 1.        ,\n",
       "       0.66274512, 0.14901961, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        ], dtype=float128)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_x[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_actual[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_logits[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit_0 = deriv_logit(batch_actual[0], all_logits[0][0])\n",
    "logit_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    print(-.001*logit_00[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First Batch\n",
    "def update_bias(pre_batch1_logits, pre_batch1_actuals, old_bias, lr = 0.001):\n",
    "    updated_biases = []\n",
    "    n_x = pre_batch1_logits.shape[0]\n",
    "    print(n_x)\n",
    "    n_output = 10\n",
    "    print(n_output)\n",
    "    for i in range(n_x):\n",
    "        batch_i = pre_batch1_logits[i]\n",
    "        print(batch_i)\n",
    "        batch_i_a = pre_batch1_actuals[i]\n",
    "        print(batch_i_a)\n",
    "        batch_i_logit = deriv_logit(pre_batch_i_a, batch_i)\n",
    "        batch_i_bias =  np.array([-lr*pre_batch_i_logit[j] for j in range(len(batch_i_logit))])\n",
    "        print(batch_i_bias)\n",
    "        #batch_i_bias = np.array([deriv_bias(batch_i_a, batch_i[j], lr) for j in range(n_output)])\n",
    "        updated_biases.append(old_bias + np.sum(batch_i_bias, axis = 0))\n",
    "    new_bias = np.sum(np.array(updated_biases), axis = 0)\n",
    "    return new_bias\n",
    "\n",
    "for i in range(len(x_train_updated)):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(x_train_updated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "update_bias(all_logits[0][0], batch_actual[0], initial_b, lr = 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_logits[0].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def deriv_bias(a, prob, lr = 0.001):\n",
    "    lr_deriv_loss_lst = []\n",
    "    n_output = 10\n",
    "    for i in range(n_output):\n",
    "        lr_deriv_loss_lst.append(i)\n",
    "        prob_i = prob[i]   \n",
    "        if a == i:\n",
    "            lr_deriv_loss_lst[i] = - lr*(prob_i - 1)\n",
    "        else:\n",
    "            lr_deriv_loss_lst[i] = - lr*(prob_i)\n",
    "    return lr_deriv_loss_lst\n",
    "\n",
    "def update_bias(X, size, Actuals, logits, old_bias, lr = 0.001):\n",
    "    updated_biases = []\n",
    "    for i in range(size):\n",
    "        batch_i = logits[size-1]\n",
    "        batch_i_a = Actuals[i]\n",
    "        batch_i_bias = np.array([deriv_bias(batch_i_a[i], batch_i[i], lr) for i in range(len(batch_i))])\n",
    "        updated_biases.append(old_bias + np.sum(batch_i_bias, axis = 0))\n",
    "    new_bias = np.sum(np.array(updated_biases), axis = 0)\n",
    "    return new_bias\n",
    "\n",
    "def update_weight(X, size, Actuals, logits, old_weights, lr = 0.001):\n",
    "    updated_weights = []\n",
    "    for i in range(size):\n",
    "        batch_i = logits[size-1]\n",
    "        batch_i_a = Actuals[i]\n",
    "        batch_i_bias = np.array([deriv_bias(batch_i_a[i], batch_i[i], lr) for i in range(len(batch_i))])\n",
    "        batch_i_weight =  np.array([batch_i_a[i]*batch_i_bias[i][j] for i in range(batch_size) for j in range(n_output)])\n",
    "        updated_weights.append(old_weights + np.sum(batch_i_weight, axis = 0))\n",
    "    new_weights = np.sum(np.array(updated_weights), axis = 0)\n",
    "    return new_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "update_bias(batch_x[0], t_batch , batch_actual[0], all_logits[0], initial_b, lr = 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "update_bias(batch_x, t_batch , batch_actual, all_logits, initial_b, lr = 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(initial_b.shape)\n",
    "updated_b = np.array(update_bias(batch_x, t_batch , batch_actual, all_logits, initial_b, lr = 0.01))\n",
    "print(updated_b.shape)\n",
    "\n",
    "sum_updated_b = np.sum(updated_b, axis = 0)\n",
    "print(sum_updated_b.shape)\n",
    "\n",
    "updated_w = np.array(update_weight(batch_x, t_batch , batch_actual, all_logits, initial_w,lr = 0.01))\n",
    "print(updated_w.shape)\n",
    "\n",
    "sum_updated_w = np.sum(updated_w, axis = 0)\n",
    "print(initial_w.shape)\n",
    "print(sum_updated_w.shape)\n",
    "\n",
    "updated_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(initial_b.shape)\n",
    "print(initial_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sum_updated_b.shape)\n",
    "print(sum_updated_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(initial_w.shape)\n",
    "print(initial_w)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# e^1000, which WolframAlpha says is a little less than 2e434. The problem is that the numbers you want to use are just too large for numpy to handle, so for odd values, you get infinity.\n",
    "print(sum_updated_w.shape)\n",
    "print(sum_updated_w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(all_logits.shape)\n",
    "print(all_logits[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "updated_logits = np.array([get_logit(sum_updated_b, sum_updated_w, batch_x[i]) for i in range(t_batch)]) #t_batch)\n",
    "print(updated_logits.shape)\n",
    "print(updated_logits[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(t_batch)\n",
    "print(batch_x.shape)   \n",
    "batch_x[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(batch_actual.shape)\n",
    "batch_actual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pred_batch_x = get_pred(updated_logits)\n",
    "print(pred_batch_x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_accuracy(pred_batch_x, batch_actual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def backprop(old_bias, old_weights, prob_exp, lr, epoch = 50):\n",
    "    for e in range(epoch):\n",
    "        updated_b = update_bias(batch_x, t_batch , batch_actual, prob_exp, old_bias, lr)\n",
    "        print(updated_b)\n",
    "        updated_w = update_weight(batch_x, t_batch , batch_actual, prob_exp, old_weights, lr)\n",
    "        new_prob_exp = np.array([get_logit(updated_b, updated_w, batch_x[i]) for i in range(t_batch)])\n",
    "        pred_batch_x = get_pred(new_prob_exp)\n",
    "        print(check_accuracy(pred_batch_x, batch_actual))\n",
    "        old_bias = updated_b\n",
    "        old_weights = updated_w\n",
    "        prob_exp = new_prob_exp\n",
    "        e =+ 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "backprop(initial_b, initial_w, all_logits, 0.001, epoch = 5)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# find the index of the element with max probability assigned that will be the prediction of the model\n",
    "#use list instead of np\n",
    "#print(np.where(updated_logits[0][0] == np.amax(updated_logits[0][0])))\n",
    "lst = []\n",
    "pred_batch_x = np.array(lst.append(list(updated_logits[i][j]).index(max(updated_logits[i][j]))) for i in range(3) for j in range(3)) \n",
    "print(pred_batch_x.shape)\n",
    "pred_batch_x\n",
    "\n",
    "def check_accuracy(predicted, expected):\n",
    "    lst = []\n",
    "    for i in range(predicted.shape[0]):\n",
    "        lst_l = []\n",
    "        for j in range(predicted.shape[1]):\n",
    "            lst_l.append(np.argmax(predicted[i][j]))\n",
    "        np.array(lst.append(lst_l))\n",
    "        max_predicted = np.array(lst)\n",
    "    batch_acc = []\n",
    "    for i in range(expected.shape[0]):\n",
    "        batch_acc.append(sklearn.metrics.accuracy_score(max_predicted[i], expected[i]))\n",
    "    return sum(batch_acc)/len(batch_acc)\n",
    "\n",
    "check_accuracy(pred_batch_x, batch_actual)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#np.seterr(divide='ignore', invalid='ignore')\n",
    "def get_logit(bias_arr, weight_mat, data_x):\n",
    "    sum_wx = np.array([np.sum(data_x*weight_mat[i], axis =1) for i in range(n_output)], dtype=np.float128)\n",
    "    print(sum_wx.shape[0])\n",
    "    sum_wxb = np.array([bias_arr[i] + sum_wx[i] for i in range(n_output)])\n",
    "    print(sum_wxb.shape)\n",
    "    sum_wxbt = np.transpose(sum_wxb)\n",
    "    print(sum_wxbt.shape)\n",
    "    print(sum_wxbt[0])\n",
    "    print(np.exp(sum_wxbt[0]))\n",
    "    sum_wxbte = np.array([np.exp(np.array(sum_wxbt[i])) for i in range(sum_wxbt.shape[0])]) \n",
    "    print(sum_wxbte.shape)\n",
    "    sum2_wxbte = np.sum(sum_wxbte, axis=1)\n",
    "    print(sum2_wxbte.shape)\n",
    "    \n",
    "    logit = np.array([sum_wxbte[i]/sum2_wxbte[i] for i in range(len(data_x))])\n",
    "    return logit\n",
    "\n",
    "get_logit(sum_updated_b, sum_updated_w, batch_x[0])\n",
    "\n",
    "#np.seterr(divide='ignore', invalid='ignore')\n",
    "bias_arr1 = sum_updated_b\n",
    "weight_mat1 = sum_updated_w\n",
    "data_x = batch_x[0]\n",
    "sum_wx = np.array([np.sum(data_x*weight_mat1[i], axis =1) for i in range(n_output)], dtype=np.float128)\n",
    "sum_wxb = np.array([bias_arr1[i] + sum_wx[i] for i in range(n_output)])\n",
    "\n",
    "sum_wxbt = np.transpose(sum_wxb)\n",
    "sum_wxbte = np.array([np.exp(np.array(sum_wxbt[i])) for i in range(sum_wxbt.shape[0])]) \n",
    "sum2_wxbte = np.sum(sum_wxbte, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array([get_logit(sum_updated_b, sum_updated_w, batch_x[0])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# must sum to 1\n",
    "print(updated_logits.shape)\n",
    "np.sum(updated_logits, axis = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "updated_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "list(updated_logits[0][1]).index(max(updated_logits[0][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lst = []\n",
    "for i in range(3):\n",
    "    for j in range(3):\n",
    "        lst.append(list(updated_logits[i][j]).index(max(updated_logits[i][j])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the index of the element with max probability assigned that will be the prediction of the model\n",
    "#use list instead of np\n",
    "#print(np.where(updated_logits[0][0] == np.amax(updated_logits[0][0])))\n",
    "lst = []\n",
    "pred_batch_x = np.array(list(updated_logits[i][j]).index(max(updated_logits[i][j])) for i in range(updated_logits.shape[0]) for j in range(updated_logits.shape[1])) \n",
    "print(pred_batch_x.shape)\n",
    "pred_batch_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_updated_logits = np.array([max(updated_logits[i][j]) for i in range(updated_logits.shape[0]) for j in range(updated_logits.shape[1])] )\n",
    "        \n",
    "#np.max(new_logits[i][j] for i in range(new_logits.shape[0]) for j in range(new_logits.shape[1]))\n",
    "\n",
    "max_updated_logits.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_updated_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sum_updated_w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(initial_w[0][0])\n",
    "print(sum_updated_w[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "updated_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.array([get_logit(sum_updated_b, sum_updated_w, batch_x[i]) for i in range(t_batch)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "t_batch = round(len(x_train_updated)/batch_size)\n",
    "all_logits = np.array([get_logit(initial_b, initial_w, batch_x[i]) for i in range(t_batch)])\n",
    "\n",
    "updated_b = np.array(update_bias(batch_x, t_batch , batch_actual, all_logits, lr = 0.1))\n",
    "sum_updated_b = np.sum(updated_b, axis = 0)\n",
    "updated_w = np.array(update_weight(batch_x, t_batch , batch_actual, all_logits, lr = 0.1))\n",
    "sum_updated_w = np.sum(updated_w, axis = 0)\n",
    "updated_logits = np.array([get_logit(updated_b, updated_w, batch_x[i]) for i in range(2)]) #t_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(check_accuracy(updated_logits, batch_actual))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_logits\n",
    "all_logits.shape\n",
    "len(batch_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_logits[599]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(batch_x.shape)\n",
    "print(batch_actual.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(updated_w[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(updated_weights[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "updated_logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_updated_logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "batch_actual.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_sum_exp = sum_exp_func(new_logits)\n",
    "new_prob_exp = prob_exp_func(new_logits, sum_exp_func(new_logits))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_updated_logits[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# old_weights is an array of length 10, where each element is a list of 784 value\n",
    "# old_bias is an array of length 10\n",
    "\n",
    "def backprop(old_weights, old_bias, prob, lr, epoch = 50):\n",
    "    #df = \n",
    "    n_output = 10\n",
    "    list_prob = []\n",
    "    y = []\n",
    "    for i in range(1,epoch+1):\n",
    "        y.append(list(range(10)))\n",
    "        #[lr_deriv_loss_func(a, prob, lr, i) for i in range(60000)]\n",
    "        lr_deriv_loss = np.array([lr_deriv_loss_func(actuals[i], sample_prob[j]) for i in range(batch_size) for j in range(n_output)])\n",
    "        \n",
    "        #delta_weights = delta_w(X_train , lr_deriv_loss)\n",
    "        \n",
    "        #new_bias = old_bias + lr_deriv_loss\n",
    "        new_bias = np.array([bias_arr + deriv_bias[i] for i in range(batch_size)])\n",
    "        #print(\"new bias is equal to old bias: \" + str(new_bias == old_bias))\n",
    "        \n",
    "        #new_weights = old_weights + delta_weights\n",
    "        new_weight = np.array([weights_mat + deriv_weight[i] for i in range(batch_size)])\n",
    "        # print(\"new weights is equal to old weights: \" + str(new_weights == old_weights))\n",
    "        new_logits = []\n",
    "        for l in range(0,10):\n",
    "            new_logits.append(l)\n",
    "            new_logits[l] = sum(new_weights[l]*mnist_img) + new_bias[l]\n",
    "        new_sum_exp = sum_exp_func(new_logits)\n",
    "        new_prob_exp = prob_exp_func(new_logits, sum_exp_func(new_logits))\n",
    "        \n",
    "        list_prob.append(new_prob_exp)\n",
    "        #print(new_prob_exp)\n",
    "        old_bias = new_bias\n",
    "        old_weights = new_weights\n",
    "        prob_exp = new_prob_exp\n",
    "    \n",
    "    return new_prob_exp\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "backprop(weights_mat, bias_arr, sample_prob, 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_actual_0 = batch_actual[0]\n",
    "batch_1_deriv = np.array([deriv_bias(batch_actual_0[i], batch_0[i]) for i in range(len(batch_0))])\n",
    "new_bias = biases_arr + np.sum(batch_1_deriv, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(batch_1_deriv.shape)\n",
    "batch_x_0.shape\n",
    "batch_x_0[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deriv_weight"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
